{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 信息论基础（熵，联合熵，条件熵，信息增益，基尼不纯度）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 信息"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$I(X=X_i)=-logP(X_i)$,信息量与事件x发生的概率成负相关。信息量是用来衡量事件的不确定性。事件发生的概率越大，不确定性越小，则它所携带的信息量就越小\n",
    "\n",
    "$I(X=X_i)$表示随机变量X取$X_i$时的信息量，$P(X_i)$表示$X_i$发生的概率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 信息熵"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "随机变量Y,取值有n个，分别是$Y_1,Y_2,...,Y_n$,对应的的概率为$P(Y_1),P(Y_2),...,P(Y_n)$,\n",
    "\n",
    "此时熵的定义$H(Y)=\\sum^{n}_{i=1}P(Y_i)I(Y_i)=-\\sum^{n}_{i=1}P(Y_i)logP(Y_i)$\n",
    "\n",
    "信息熵用于衡量随机变量的不确定性；很明显熵计算的是期望。系统越复杂，出现不同情况的种类越多，携带的信息就越多，熵就越大\n",
    "\n",
    "信息增益是针对某一特征变量X，有X没有X之间信息量的差值。\n",
    "\n",
    "$H(Y)$：包含所有特征的信息量，$H(Y\\mid X)$:不包含特征X的信息量\n",
    "\n",
    "确定一件事：log底数随意，通常是2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 联合熵"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "联合熵度量联合分布的不确定性\n",
    "\n",
    "$H(X,Y)=-\\sum_{x\\in X}\\sum_{y\\in Y}P(x,y)logP(x,y)$\n",
    "\n",
    "$=-\\sum_{x\\in X}\\sum_{y\\in Y}P(x,y)log[P(x)P(y\\mid x)]$\n",
    "\n",
    "$=-\\sum_{x\\in X}P(x)logP(x)-\\sum_{x\\in X}\\sum_{y\\in Y}P(x,y)logP(y\\mid x)$\n",
    "\n",
    "$=H(X)+H(Y\\mid X)$\n",
    "\n",
    "$=H(Y)+H(X\\mid Y)$\n",
    "\n",
    "当X,Y相互独立，$H(X,Y)=H(X)+H(Y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 条件熵"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "问题1：为什么$H(Y\\mid X)$是不包含特征X的信息量？\n",
    "\n",
    "我们把特征X固定，作为已知条件，X不再变化，等价于不包含特征X\n",
    "\n",
    "问题2：怎么把特征X固定？\n",
    "\n",
    "特征X取值${X_1,X_2,...,X_m}$,概率${P(X_1),P(X_2),...,P(X_m)}$，每个$X_i$对应的条件熵都计算，最后以概率为权重求和。\n",
    "\n",
    " $H(Y\\mid X)=P(X_1)H(Y\\mid X_1)+P(X_2)H(Y\\mid X_2)+...+P(X_m)H(Y\\mid X_m)$\n",
    "\n",
    "$=\\sum^m_{k=1}P(X_k)H(Y\\mid X_k)$\n",
    "\n",
    "$=-\\sum^m_{k=1}P(X_k)\\sum^{n}_{i=1}P(Y_i\\mid X_k)logP(Y_i\\mid X_k)$\n",
    "\n",
    "$=-\\sum_{x\\in X,y\\in Y}P(x,y)logP(y\\mid x)$\n",
    "\n",
    "$H(Y\\mid X)=H(X,Y)-H(X)$\n",
    "\n",
    "$H(Y\\mid X)$表示特征X所有值固定后的条件熵，$H(Y\\mid X_k)$表示特征$X=X_k$时的条件熵,m表示特征X的取值数量，n表示类别Y的数量\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 信息增益(Information Gain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "信息增益=信息熵-条件熵 \n",
    "\n",
    "$IG(X)=H(Y)-H(Y\\mid X)$\n",
    "\n",
    "计算出的条件熵相比原先信息熵更小，也就是说，引入新变量后事件的不确定性减小。减小的部分即信息增益。\n",
    "\n",
    "在分类问题中，已知特征x后类别y的信息熵减小，即更容易确定类别y。因此信息增益大的特征分类能力更强\n",
    "\n",
    "信息增益比=信息增益/信息熵(对应特征)\n",
    "\n",
    "小结：\n",
    "\n",
    "$H(Y)=-\\sum^{n}_{i=1}P(Y_i)logP(Y_i)$\n",
    "\n",
    "$H(Y\\mid X)=-\\sum^m_{k=1}P(X_k)\\sum^{n}_{i=1}P(Y_i\\mid X_k)logP(Y_i\\mid X_k)$\n",
    "\n",
    "$IG(X)=H(Y)-H(Y\\mid X)$说明：m表示特征X的取值数量，n表示类别Y的数量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基尼不纯度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据集D有个类别变量Y,取值有n个，分别是$Y_1,Y_2,...,Y_n$,对应的的概率为$P_1,P_2,...,P_n$,\n",
    "\n",
    "$Gini(D)=1-\\sum^n_{i=1}P(i)$，$Gini(D)$表示数据集D的基尼不纯度\n",
    "\n",
    "$Gini(D,A)=\\frac{D_1}{D}Gini(D_1)+\\frac{D_2}{D}Gini(D_2)$，$Gini(D，A)$表示基于特征A将数据集D分成$D_1,D_2$后的基尼不纯度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 相对熵"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "相对熵(KL散度)：衡量两个分布之间的距离。\n",
    "\n",
    "$D_{KL}(p||q)=\\sum p(x)log\\frac{p(x)}{q(x)}$\n",
    "\n",
    "$=\\sum p(x)logp(x)-\\sum p(x)logp(x)$\n",
    "\n",
    "$=-H(p)+H_p(q)$，\n",
    "\n",
    "真实为P,假设为q,$0log\\frac{0}{0}=0,0log\\frac{0}{q}=0,plog\\frac{p}{0}=\\infty$,\n",
    "\n",
    "$p=q$时，$D_{KL}(p||q)=0$,$H_p(q)$表示在p分布下，使用q进行编码需要的bit数;$H(p)$表示对真实分布p所需要的最小编码bit数,KL散度即多出来的编码数\n",
    "\n",
    "逻辑回归中使用的交叉熵与KL散度之间的关系：$H_p(q)=H(p)+D_{KL}(p||q)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 互信息"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "互信息定义为X，Y的联合分布和各自独立分布乘积的相对熵\n",
    "\n",
    "$I(X,Y)=\\sum_{x,y}p(x,y)log\\frac{p(x,y)}{p(x)p(y)}$\n",
    "\n",
    "$H(Y)-I(X,Y)$\n",
    "\n",
    "$=-\\sum_y \\sum_x p(x,y)logp(y)-\\sum_{x,y}p(x,y)log\\frac{p(x,y)}{p(x)p(y)}$\n",
    "\n",
    "$=-\\sum_{x,y}p(x,y)logp(y)-\\sum_{x,y}p(x,y)log\\frac{p(x,y)}{p(x)p(y)}$\n",
    "\n",
    "$=-\\sum_{x,y}p(x,y)log\\frac{p(x,y)}{p(x)}=H(Y|X)$\n",
    "\n",
    "互信息好像和信息增益是一个东西？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 决策树的不同分类算法的原理及应用场景（ID3,C4.5,CART分类树）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 决策树概述"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "决策树是一个树结构。每个非叶节点表示某个特征属性，每个分支代表这个特征属性在某个值域上的输出，而每个叶节点存放一个类别。\n",
    "\n",
    "使用决策树进行决策就是从根节点开始，测试待分类项中相应的特征属性，并按照其值选择输出分支，直到到达叶子节点，将叶子节点存放的类别作为决策结果。\n",
    "\n",
    "决策树的本质就是从训练集中归纳出一组分类规则。换句话说，类似于if,else语句 里面判断条件就是对特征属性的测试。return的就是决策结果。\n",
    "\n",
    "决策树的通用步骤是特征选择、决策树的生成、决策树的修剪。\n",
    "\n",
    "目的：让各个分裂子集尽可能地“纯”，即让一个分裂子集中待分类项属于同一类别。\n",
    "\n",
    "分类属性可能的情况\n",
    "\n",
    "1.属性离散且不要求生成二叉树。此时属性的每个划分作为一个分支。\n",
    "\n",
    "2.属性离散且要求生成二叉树。此时按照“属于此子集”和“不属于此子集”分成两个分支。\n",
    "\n",
    "3.属性连续。此时确定一个值作为分裂点split_point，按照>split_point和<=split_point生成两个分支。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ID3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "生成树的基准：信息增益。哪个特征的信息增益大，选择哪个特征生成二叉树\n",
    "\n",
    "具体过程：\n",
    "\n",
    "1.根节点开始，计算所有特征的信息增益，选择增益最大的特征作节点\n",
    "\n",
    "2.根据特征的不同取值建立子节点，再回到1，递归直到没有特征或者结果增益很小\n",
    "\n",
    "$H(Y)=-\\sum^{n}_{i=1}P(Y_i)logP(Y_i)$\n",
    "\n",
    "$H(Y\\mid X)=-\\sum^m_{k=1}P(X_k)\\sum^{n}_{i=1}P(Y_i\\mid X_k)logP(Y_i\\mid X_k)$\n",
    "\n",
    "$IG(X)=H(Y)-H(Y\\mid X)$说明：m表示特征X的取值数量，n表示类别Y的数量\n",
    "\n",
    "分类属性连续：将该属性排序，选择分类点，求分类后的两个集合的期望信息。\n",
    "\n",
    "期望信息越小，结果越确定。最终选择最小的分裂点，按照>split_point和<=split_point生成两个分支。\n",
    "\n",
    "适用场合：二分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C4.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ID3算法存在的问题：分裂属性倾向于多值属性，如身份证信息，结果很纯，但是毫无意义\n",
    "\n",
    "C4.5生成树的基准：信息增益比\n",
    "\n",
    "信息增益比=信息增益/信息熵(对应特征)\n",
    "\n",
    "克服ID3算法存在的分裂属性倾向问题,其他过程和ID3算法完全一致。\n",
    "\n",
    "tips:理想情况下一个分裂子集中待分类项属于同一类别。实际过程中，决策结果如下$(0,0,0,0,1,1)$,以投票原则，该节点输出类别为0.\n",
    "\n",
    "C4.5适用场合：非离散数据，数据不完整\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CART"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CART生成树的基准：Gini 指数\n",
    "\n",
    "CART全称是分类与回归树，CART不需要对数运算更高效，生成的树必须是二叉树\n",
    "\n",
    "适用场合：分类问题和回归问题，连续数据\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 回归树原理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://blog.csdn.net/weixin_40604987/article/details/79296427\n",
    "\n",
    "算法：\n",
    "\n",
    "1.选择最优的切分变量J和切分点S,求解\n",
    "\n",
    "$min_{j,s}[min_{C_1}\\sum_{x_i\\in R_1(j,s)}(y_i-c_1)^2+min_{C_2}\\sum_{x_i\\in R_2(j,s)}(y_i-c_2)^2]$\n",
    "\n",
    "2 用选定的划分区域计算输出值：当$x^{(j)}\\le s$为$R_1$,否则为$R_2$,计算域内平均值$\\widehat{c}_m$，回到1，直至不再可分\n",
    "\n",
    "3.输入空间划分为M个区域$R_1,R_2,...,R_M$,生成决策树\n",
    "\n",
    "$f(x)=\\sum^M_{m=1}\\widehat{c}_mI(x_i\\in R_m)$\n",
    "\n",
    "\n",
    "\n",
    "选择切分变量J,这里指用于分类的不同特征，假设有m种特征可以拿来分类，如{性别，体重}，现在如果选好了切分变量为体重，所有人的体重必然在一个区间$[w_min,w_max]$。现在选择切分点T。切分点T将区域化为2块，这两块区域分别计算loss,求和。不同T对应不同的loss,找到最小loss对应的切分点。此时区域被划分。但是可能存在新的切分点，能继续分割区域，直至不再可分。上面只是计算了切分变量体重对应的损失，再计算性别的损失，最终谁的loss小，选择对应的切分变量和切分点。\n",
    "\n",
    "回归树结果：像是分段函数，阶梯型\n",
    "\n",
    "发展：回归树到提升树（Boosting Decision Tree），再到梯度提升树（Gradient Boosting Decision Tree，GBDT），再进一步可以升级到XGBoost。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 决策树防止过拟合的手段"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "过拟合通常表现为决策树深度太深，子节点众多\n",
    "\n",
    "决策树的损失函数：$C_\\alpha(T)=\\sum^{|T|}_{t=1}N_tH_t(T)+\\alpha|T|$\n",
    "\n",
    "$|T|$表示叶子节点数量，$N_t$表示第t个叶子节点对应的样本数量，$H_t(T)$表示第t个叶子节点的熵\n",
    "\n",
    "$\\alpha$越大，惩罚越大，树越简单"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 剪枝"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "按剪枝顺序分为两类\n",
    "\n",
    "1.在构造过程中，当某个节点满足剪枝条件，则直接停止构造此分支。\n",
    "\n",
    "2.构造完整的决策树，再通过某些条件遍历树进行剪枝。\n",
    "\n",
    "这里的某些条件就是剪枝前后泛化能力是否提高，提高就剪了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型评估"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "决策树适用分类指标：P,R,F1,ROC曲线,使用交叉验证"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回归树适合回归指标：MAE，MSE，RMSE，R平方，Adjusted R平方，MAPE，RMSPE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sklearn参数详解，python绘制决策树"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## class sklearn.tree.DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class sklearn.tree.DecisionTreeClassifier（criterion ='gini'，splitter ='best'，max_depth = None，min_samples_split = 2，min_samples_leaf = 1，min_weight_fraction_leaf = 0.0，max_features = None，random_state = None，max_leaf_nodes = None，min_impurity_decrease = 0.0，min_impurity_split = None，class_weight = None，presort = False ）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "criterion：特征选择标准，可选参数，默认是gini\n",
    "\n",
    "splitter：best是根据算法选择最佳的切分特征。random找局部最优的划分点。\n",
    "\n",
    "max_depth:树的最大深度。None表示划分到所有叶子结点都是纯的或小于最小样本量\n",
    "\n",
    "min_samples_split ：节点可以划分的最小样本量 \n",
    "\n",
    "min_samples_leaf：叶子节点所需的最小样本数。\n",
    "\n",
    "min_weight_fraction_leaf ：计算权重，默认大家权重一样\n",
    "\n",
    "max_features ：寻找最佳分割时考虑的特征数量\n",
    "\n",
    "random_state：随机数种子\n",
    "\n",
    "max_leaf_node: 最大的叶节点数量\n",
    "\n",
    "min_impurity_decrease：节点划分最小不纯度，小于该值不再分裂\n",
    "\n",
    "min_impurity_split：节点的不纯度高于该值，才进行分裂\n",
    "\n",
    "class_weight：类别权重\n",
    "\n",
    "presort ：数据是否预排序，默认False,数据量小可以改成True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## apply(X, check_input=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输入：X为形如[样本数，特征数]的输入样本，chech_input表示允许绕过多个输入检查。\n",
    "\n",
    "输出：形如[样本数]，预测样本所在叶子节点的索引值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## decision_path（X，check_input = True ）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输入：X为形如[样本数，特征数]的输入样本，chech_input表示允许绕过多个输入检查。\n",
    "\n",
    "输出：形如[样本数，节点数]，预测样本所在叶子节点 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T07:10:27.865897Z",
     "start_time": "2019-03-05T07:10:27.861933Z"
    }
   },
   "source": [
    "## feature_importances_：返回每个特征的重要性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fit(X, y, sample_weight=None, check_input=True, X_idx_sorted=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练模型\n",
    "\n",
    "X为形如[样本数，特征数]的输入样本，y形如[样本数]或[样本数，输出]，sample_weight 为样本权重\n",
    "\n",
    "chech_input表示允许绕过多个输入检查。X_idx_sorted：默认对训练输入样本的索引排序\n",
    "\n",
    "返回：self : object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_param(deep=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "获取当前模型及子项目的参数\n",
    "\n",
    "deep布尔值：True,返回参数\n",
    "\n",
    "返回值：参数名到对应值的映射字符串"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predict(X,check_input=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T07:19:39.407219Z",
     "start_time": "2019-03-05T07:19:39.401237Z"
    }
   },
   "source": [
    "X:形如[样本数，特征数]的数据，chech_input表示允许绕过多个输入检查。\n",
    "    \n",
    "返回预测类别或预测值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predict_log_proba(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "预测概率值取log\n",
    "\n",
    "X:形如[样本数，特征数]的数据\n",
    "\n",
    "返回值T:根据标签排序，返回所有类别预测概率值的log值,形如[样本数，类别数]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predict_proba(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "预测概率值\n",
    "\n",
    "X:形如[样本数，特征数]的数据\n",
    "\n",
    "返回值T:根据标签排序，返回所有类别预测概率值,形如[样本数，类别数]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## score(X, y, sample_weight=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于多标签分类问题，子集准确率使用哈希尺度，要求每个样本的各个类别都能正确预测\n",
    "\n",
    "X:形如[样本数，特征数]的测试数据\n",
    "    \n",
    "y:形如[样本数，标签数]的数据的真实值\n",
    "    \n",
    "样本权重：每个样本有独立的权重\n",
    "\n",
    "返回平均准确率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## set_params(** params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设置参数\n",
    "\n",
    "返回实例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## python绘制决策树"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.datasets import load_iris \n",
    "\n",
    "from sklearn import tree \n",
    "\n",
    "iris = load_iris()#加载数据\n",
    "\n",
    "clf = tree.DecisionTreeClassifier()#初始化模型\n",
    "\n",
    "clf = clf.fit(iris.data, iris.target)#训练模型\n",
    "\n",
    "with open(\"iris.dot\", 'w') as f:\n",
    "    f = tree.export_graphviz(clf, out_file=f)#存储决策树\n",
    "    \n",
    "#第一种方法\n",
    "\n",
    "import os \n",
    "os.unlink('iris.dot')\n",
    "\n",
    "dot_data = tree.export_graphviz(clf, out_file=None)\n",
    "\n",
    "#第二种方法\n",
    "\n",
    "import pydotplus\n",
    "\n",
    "graph = pydotplus.graph_from_dot_data(dot_data)\n",
    "\n",
    "graph.write_pdf(\"iris.pdf\")\n",
    "\n",
    "#画图\n",
    "\n",
    "from IPython.display import Image  \n",
    "\n",
    "dot_data = tree.export_graphviz(clf, out_file=None, feature_names=iris.feature_names, class_names=iris.target_names,  filled=True,rounded=True,  special_characters=True) \n",
    "\n",
    "graph = pydotplus.graph_from_dot_data(dot_data)  \n",
    "\n",
    "Image(graph.create_png())  \n",
    "\n",
    "clf.predict(iris.data[:1, :])\n",
    "#array([0])\n",
    "\n",
    "clf.predict_proba(iris.data[:1, :])\n",
    "#array([[ 1.,  0.,  0.]])\n",
    "\n",
    "#参考链接：https://blog.csdn.net/Yeoman92/article/details/73436632"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
